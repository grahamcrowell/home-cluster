---
version: "3"

tasks:
  cluster_status:
    desc: Cluster status
    summary: Determine status of the cluster
    ignore_error: true
    cmds:
      - for: { var: NODE_NUMBERS }
        task: node_check:{{ .ITEM }}
      - talosctl --nodes cp0 health --wait-timeout=1s
      - kubectl get nodes
      - kubectl get pods --all-namespaces
      - kubectl get services --all-namespaces
      - kubectl get secrets --all-namespaces
      - kubectl -n rook-ceph describe cephcluster
      - kubectl --namespace rook-ceph get cephclusters.ceph.rook.io rook-ceph

  node_check:*:
    desc: Node status
    summary: Determine status of the node
    ignore_error: false
    silent: true
    internal: true
    vars:
      N: "{{index .MATCH 0}}"
      NODE_IP: "192.168.4.1{{ .N }}"
      NODE: "cp{{ .N }}"
    cmds:
      - printf "\nChecking node {{ .NODE }}\n"
      - echo "Pinging node {{ .NODE_IP }}"
      - ping -t 1 -c 1 {{ .NODE_IP }} | grep '1 packets received, 0.0% packet loss' >/dev/null && echo "OK - Ping successful" || { echo "ERROR - Node {{ .NODE_IP }} unreachable"; exit 1; }
      - echo "Checking if config applied"
      - talosctl -n {{ .NODE_IP }} dmesg >/dev/null 2>&1 && echo "OK - talos config has been applied to {{ .NODE_IP }}" || { echo "ERROR - talos config has not been applied to {{ .NODE_IP }}"; exit 1; }

  nuke_and_pave:
    desc: Nuke all nodes
    summary: Reset and wipe all nodes on the cluster
    deps:
      - nuke_node:0
      - nuke_node:1
      - nuke_node:2

  nuke_node:*:
    internal: true
    desc: Nuke node
    ignore_error: true
    vars:
      N: "{{index .MATCH 0}}"
      NODE_IP: "192.168.4.1{{ .N }}"
    cmds:
      - echo "Nuking node {{ .NODE_IP }}"
      - talosctl reset --nodes {{ .NODE_IP }} --endpoints {{ .NODE_IP }} --talosconfig={{ .TALOSCONFIG }} --debug --graceful=false

  talos_generate_configs:
    dir: "{{.TALOS_DIR}}"
    desc: Generate configs
    prompt: (re)generate configs?
    cmds:
      - op inject -i talsecret.op.yaml -o talsecret.yaml --force
      - talhelper genconfig --config-file talconfig.yaml --secret-file talsecret.yaml
      - rm talsecret.yaml
      - cp {{ .TALOSCONFIG }} ~/.talos/config
    preconditions:
      - op user get --me
      - which talosctl talhelper op
    status:
      - test -f {{ .TALOSCONFIG }}
      - test -f clusterconfig/boondoggle-cp0.yaml
      - test -f clusterconfig/boondoggle-cp1.yaml
      - test -f clusterconfig/boondoggle-cp2.yaml
  
  talos_nodes:
    dir: "{{.TALOS_DIR}}"
    desc: Bootstrap Talos nodes
    prompt: Bootstrap Talos nodes?
    cmds:
      - task: talos_apply_config_cp0
      - task: talos_bootstrap_cp0
      - task: talos_apply_config_cp1_cp2

  talos_apply_config_cp0:
    dir: "{{.TALOS_DIR}}"
    desc: Apply Talos config to cp0
    prompt: Apply Talos config to cp0?
    preconditions:
      - which talosctl
      - "curl -m 1 {{ .INIT_NODE_IP }}:50000 2>&1 | grep 'curl: (52) Empty reply from server' > /dev/null"
    status:
      - talosctl -n {{ .INIT_NODE_IP }} dmesg | grep 'please run `talosctl bootstrap`' > /dev/null
    cmds:
      - task: talos_apply_config:0
      - until talosctl -n {{ .INIT_NODE_IP }} dmesg | grep 'please run `talosctl bootstrap`' > /dev/null; do sleep 5; done

  talos_apply_config_cp1_cp2:
    aliases:
      - talos
    dir: "{{.TALOS_DIR}}"
    desc: Apply Talos config to cp1 and cp2
    prompt: Apply Talos config to cp1 and cp2?
    deps:
      - talos_apply_config:1
      - talos_apply_config:2
    preconditions:
      - which talosctl

  talos_apply_config:*:
    dir: "{{ .TALOS_DIR }}"
    desc: Apply Talos config to node NODE_IP
    preconditions:
      - which talosctl
      - test -f clusterconfig/boondoggle-cp{{ .N }}.yaml
      - ping -c 1 {{ .NODE_IP }} | grep '1 packets received, 0.0% packet loss'
    status:
      - talosctl -n {{ .NODE_IP }} dmesg >/dev/null 2>&1 && echo "talos config has been applied to {{ .NODE_IP }}"
    vars:
      N: "{{index .MATCH 0}}"
      NODE_IP: "192.168.4.1{{ .N }}"
      CONFIG_FILE: clusterconfig/boondoggle-cp{{ .N }}.yaml
    cmds:
      - echo "Applying Talos config to node cp{{ .N }}"
      - talosctl apply-config --nodes {{ .NODE_IP }} --insecure --file {{ .CONFIG_FILE }}
      - until talosctl -n {{ .NODE_IP }} dmesg >/dev/null 2>&1; do echo "waiting for node {{ .NODE_IP }}"; sleep 5; done

  talos_bootstrap_cp0:
    dir: "{{.TALOS_DIR}}"
    desc: Bootstrap Talos on cp0
    prompt: Bootstrap Talos on cp0?
    status:
      - talosctl -n {{ .INIT_NODE_IP }} dmesg | grep 'bootstrap request received' > /dev/null
      - talosctl -n {{ .INIT_NODE_IP }} dmesg | grep 'created /v1/Service/talos' > /dev/null
    deps:
      - talos_apply_config_cp0
    cmds:
      - "until talosctl -n {{ .INIT_NODE_IP }} dmesg | grep 'please run `talosctl bootstrap`' > /dev/null; do sleep 5; done"
      - talosctl bootstrap --nodes {{ .INIT_NODE_IP }} --endpoints {{ .INIT_NODE_IP }} --talosconfig={{ .TALOSCONFIG }}
      - until talosctl -n {{ .INIT_NODE_IP }} dmesg | grep 'created /v1/Service/talos' > /dev/null; do echo "waiting for created /v1/Service/talos log"; sleep 5; done
      - talosctl kubeconfig --nodes {{ .INIT_NODE_IP }} --endpoints {{ .INIT_NODE_IP }} --talosconfig={{ .TALOSCONFIG }} --force
    preconditions:
      - op user get --me
      - which talosctl
      - ping -t 1 -c 1 {{ .INIT_NODE_IP }} | grep '1 packets received, 0.0% packet loss' >/dev/null
      - talosctl -n {{ .INIT_NODE_IP }} dmesg | grep 'please run `talosctl bootstrap`' > /dev/null

  # 1Password tasks
  1password:
    desc: Create new 1Password connect token named {{ .OP_CONNECT_TOKEN_LABEL }} on server named {{ .OP_CONNECT_SERVER }} and save to field {{ .OP_CONNECT_TOKEN_LABEL }} in {{ .OP_CONNECT_ITEM }} item
    prompt: Create new 1Password connect token named {{ .OP_CONNECT_TOKEN_LABEL }} on server named {{ .OP_CONNECT_SERVER }} and save to field {{ .OP_CONNECT_TOKEN_LABEL }} in {{ .OP_CONNECT_ITEM }} item?
    deps:
      - connect_server
    status:
      - op connect token list --format json | jq --exit-status '.[] | select(.name=="{{ .OP_CONNECT_TOKEN_LABEL }}")'
    cmds: 
      - token=$(op connect --vault {{ .OP_VAULT }} token create {{ .OP_CONNECT_TOKEN_LABEL }} --server {{ .OP_CONNECT_SERVER }}) && op --vault {{ .OP_VAULT }} item edit --category "Password" --title {{ .OP_CONNECT_ITEM }} "{{ .OP_CONNECT_TOKEN_LABEL }}[Text]=$token"
    preconditions:
      - op user get --me
      - op vault list --format=json | jq --exit-status '.[] | select(.name=="{{ .OP_VAULT }}")'
      - op connect server get {{ .OP_CONNECT_SERVER }}
      - op --vault {{ .OP_VAULT }} item list --format=json | jq --exit-status '.[] | select(.title=="{{ .OP_CONNECT_ITEM }}")'
      - sh: |
            [[ $(op connect token list --format json | jq '[.[] | select(.name=="{{ .OP_CONNECT_TOKEN_LABEL }}")] | length') -le 1 ]]
        # msg: "ERROR: Duplicate 1Password connect tokens named {{ .OP_CONNECT_TOKEN_LABEL }} found. Please remove them first."
  connect_server:
    desc: Create new 1Password connect server named {{ .OP_CONNECT_SERVER }} and item named {{ .OP_CONNECT_ITEM }}
    prompt: Create new 1Password connect server named {{ .OP_CONNECT_SERVER }} and item named {{ .OP_CONNECT_ITEM }}?
    status:
      - op connect server get {{ .OP_CONNECT_SERVER }}
      - op --vault {{ .OP_VAULT }} item get {{ .OP_CONNECT_ITEM }} --fields label={{ .OP_CONNECT_CREDENTIALS_LABEL }} > /dev/null
    cmds:
      - op connect server create {{ .OP_CONNECT_SERVER }}
      - until op connect server list --format json | jq --exit-status '.[] | select(.name=="{{ .OP_CONNECT_SERVER }}")'; do echo "waiting for connect server"; sleep 1; done
      - op --vault {{ .OP_VAULT }} item create --category "Password" --title {{ .OP_CONNECT_ITEM }} "{{ .OP_CONNECT_CREDENTIALS_LABEL }}[Text]=$(cat 1password-credentials.json | base64)"
    preconditions:
      - op user get --me
      - op vault list --format=json | jq --exit-status '.[] | select(.name=="{{ .OP_VAULT }}")'
      - sh: |
            [[ $(op connect server list --format json | jq '[.[] | select(.name=="{{ .OP_CONNECT_SERVER }}")] | length') -le 1 ]]
        # msg: "Duplicate 1Password connect servers named {{ .OP_CONNECT_SERVER }} found. Please remove them first."
      - sh: |
            [[ $(op --vault {{ .OP_VAULT }} item list --format json | jq '[.[] | select(.title == "{{ .OP_CONNECT_ITEM }}")] | length') -le 1 ]]
        # msg: "Duplicate 1Password document items named {{ .OP_CONNECT_ITEM }} found. Please remove them first."
  delete_connect:
    desc: Delete 1Password connect server named {{ .OP_CONNECT_SERVER }}, token named {{ .OP_CONNECT_TOKEN_LABEL }} and item named {{ .OP_CONNECT_ITEM }}
    prompt: Delete 1Password connect server named {{ .OP_CONNECT_SERVER }}, token named {{ .OP_CONNECT_TOKEN_LABEL }} and item named {{ .OP_CONNECT_ITEM }}?
    cmds:
      - op connect server list --format json | jq -r '.[].id' | xargs -I'{}' op connect server delete '{}'
      - op connect token list --format json | jq '.[] | select(.name=="{{ .OP_CONNECT_TOKEN_LABEL }}") | .id' | xargs -I'{}' op connect token delete '{}'
      - op --vault {{ .OP_VAULT }} item list --format json | jq '.[] | select(.title=="{{ .OP_CONNECT_ITEM }}") | .id' | xargs -I'{}' op --vault {{ .OP_VAULT }} item delete '{}'
    preconditions:
      - op user get --me
  status_connect:
    desc: List connect server {{ .OP_CONNECT_SERVER }}, token, and items
    cmds:
      - op connect server list --format json | jq --exit-status '.[] | select(.title=="{{ .OP_CONNECT_SERVER }}")' || echo "Connect server {{ .OP_CONNECT_SERVER }} not found"
      - op connect token list --format json | jq --exit-status '.[] | select(.name=="{{ .OP_CONNECT_TOKEN_LABEL }}")' || echo "Connect token {{ .OP_CONNECT_TOKEN_LABEL }} not found"
      - op --vault {{ .OP_VAULT }} item get {{ .OP_CONNECT_ITEM }} || echo "Item {{ .OP_CONNECT_ITEM }} not found"
    preconditions:
      - op user get --me

  zero_rook_drive:*:
    desc: Zero Rook drive
    vars:
      N: "{{index .MATCH 0}}"
      NODE: "cp{{ .N }}"
      DEVICE_ID: 
        sh: talosctl --nodes cp0 get disks --output json | jq -r --slurp '.[] | select(.spec.model =="{{ .ROOK_DISK }}") | .metadata.id'
    cmds:
      - echo "Zeroing rook-ceph drive with device ID {{ .DEVICE_ID }} on node {{ .NODE }}"
      - talosctl --nodes {{ .NODE }} wipe disk {{ .DEVICE_ID }} --method ZEROES &
      - talosctl --nodes {{ .NODE }} dmesg
    requires:
      vars: [ROOK_DISK]
    preconditions:
      - talosctl config info
      - talosctl --nodes {{ .NODE }} dmesg > /dev/null 2>&1

  # Kubernetes tasks
  k8s_apps:
    desc: Bootstrap Kubernetes Apps [ROOK_DISK=required]
    prompt: Bootstrap Kubernetes Apps [ROOK_DISK=required]?
    deps:
      - 1password
      # TODO: need fast status check talosctl bootstrap
    cmds:
      - kubectl config set-cluster {{ .CONTEXT }} --server https://{{ .RANDOM_CONTROLLER }}:6443
      - defer: talosctl kubeconfig --nodes {{ .RANDOM_CONTROLLER }} --force {{ .CLUSTER_DIR }}
      - echo {{ .ROOK_DISK }}
      - echo {{ .BLOCK_DEVICES_RESPONSE }}
      - echo {{ .BLOCK_DEVICES }}
      - for: { var: BLOCK_DEVICES }
        cmd: |
          echo "Wiping disk {{ .ITEM }} on {{ .KEY }}"
          talosctl --nodes {{ .KEY }} wipe disk {{ .ITEM }}
      - op inject -i  {{ .BOOTSTRAP_KUBERNETES_DIR }}/resources.yaml | kubectl apply --server-side --filename -
      - helmfile --file {{ .BOOTSTRAP_KUBERNETES_DIR }}/helmfile.yaml apply --skip-diff-on-install --suppress-diff
    requires:
      vars: [ROOK_DISK]
    vars:
      BLOCK_DEVICES_FILTER: |-
        map(select(.spec.model == "{{ .ROOK_DISK }}"))
          | group_by(.node)
          | map({ (.[0].node): (map(.metadata.id) | join(" ")) })
          | add
      BLOCK_DEVICES_RESPONSE:
        sh: talosctl get disks --output json | jq --compact-output --slurp '{{ .BLOCK_DEVICES_FILTER }}'
      BLOCK_DEVICES:
        ref: fromJson .BLOCK_DEVICES_RESPONSE
      CONTEXT:
        sh: talosctl config info --output json | jq --raw-output '.context'
      RANDOM_CONTROLLER:
        sh: talosctl config info --output json | jq --raw-output '.endpoints[]' | shuf -n 1
    preconditions:
      - op user get --me
      - talosctl config info
      - talosctl --nodes {{ .RANDOM_CONTROLLER }} get machineconfig
      - test -f {{ .BOOTSTRAP_KUBERNETES_DIR }}/helmfile.yaml
      - test -f {{ .BOOTSTRAP_KUBERNETES_DIR }}/resources.yaml
      - which helmfile jq kubectl op talosctl
